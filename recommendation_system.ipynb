{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7332ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.21.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.8 MB)\n",
      "     |████████████████████████████████| 15.8 MB 7.9 MB/s            \n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.21.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31355213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkConf\n",
    "import numpy\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5797e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r elasticsearch-hadoop-7.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc5dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data\n",
    "!apt-get install -y wget\n",
    "\n",
    "#Review (14GB)\n",
    "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Appliances.json.gz\n",
    "\n",
    "#Product metadata (12GB)\n",
    "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_Appliances.json.gz\n",
    "    \n",
    "#Download ES jar for Spark\n",
    "!wget https://repo1.maven.org/maven2/org/elasticsearch/elasticsearch-spark-30_2.12/7.12.0/elasticsearch-spark-30_2.12-7.12.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33d58ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 14:54:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initializing spark context\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars ./elasticsearch-spark-30_2.12-7.12.0.jar pyspark-shell'\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.es.nodes\",\"elasticsearch\")\n",
    "conf.set(\"spark.es.port\",\"9200\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"recommendation_system\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e082791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "# function to read data and convert to spark dataframe     \n",
    "def getMetaData(path):\n",
    "    data = []\n",
    "    data_schema =  [\n",
    "                       StructField(\"asin\", StringType(), True),\n",
    "                       StructField(\"title\", StringType(), True),\n",
    "                       StructField(\"brand\", StringType(), True),\n",
    "                       StructField(\"category\", ArrayType(StringType(), True), True),\n",
    "                       StructField(\"main_category\", StringType(), True),\n",
    "#                        StructField(\"image\", ArrayType(StringType(), True), True)\n",
    "                   ]\n",
    "    final_schema = StructType(fields=data_schema)\n",
    "    for d in parse(path):\n",
    "        review = {}\n",
    "        review['asin'] = d['asin']\n",
    "        review['title'] = d['title']\n",
    "        review['brand'] = d['brand']\n",
    "        review['category'] = d['category']\n",
    "        review['main_category'] = next(reversed(d['category']), None) if len(d['category'])!= 0 else ''\n",
    "#         review['image'] = d['image']\n",
    "        data.append(review)\n",
    "    return spark.createDataFrame(data, schema=final_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eadfbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = getMetaData('meta_Appliances.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f45f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_data = product_data.dropDuplicates(['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dffad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 14:54:27 WARN TaskSetManager: Stage 0 contains a task of very large size (2384 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+--------------------+---------------+\n",
      "|      asin|               title| brand|            category|  main_category|\n",
      "+----------+--------------------+------+--------------------+---------------+\n",
      "|B000BEZV7M|Extech RH401 Trip...|Extech|[Appliances, Part...|Humidity Meters|\n",
      "+----------+--------------------+------+--------------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "product_data.show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac22e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.9/dist-packages (7.15.1)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from elasticsearch) (1.26.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from elasticsearch) (2021.10.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd580f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DIM = 25\n",
    "\n",
    "# Index mapping\n",
    "product_mapping = {\n",
    "    # this mapping definition sets up the metadata fields for the products\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"asin\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"brand\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"category\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"main_category\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            # the following fields define our model factor vectors and metadata\n",
    "            \"model_factor\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\" : VECTOR_DIM\n",
    "            },\n",
    "            \"model_version\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"model_timestamp\": {\n",
    "                \"type\": \"date\"\n",
    "            }          \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5c0c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    ['elasticsearch'],\n",
    "    scheme=\"http\",\n",
    "    port=9200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ES index (1st time)\n",
    "es.indices.create(index=\"products\", body=product_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e8ef856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 6,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 0, 'relation': 'eq'},\n",
       "  'max_score': None,\n",
       "  'hits': []}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.search(index=\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa31a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 14:54:39 WARN TaskSetManager: Stage 2 contains a task of very large size (2384 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Indexing data to ES\n",
    "product_data.write.format(\"es\").option(\"es.mapping.id\", \"asin\").save(\"products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9a210",
   "metadata": {},
   "source": [
    "### Preparing feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63e3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read rating data\n",
    "def getRatingData(path):\n",
    "    data = []\n",
    "    data_schema = [\n",
    "               StructField(\"asin\", StringType(), True),\n",
    "               StructField(\"reviewerId\", StringType(), True),\n",
    "               StructField(\"rating\", FloatType(), True)]\n",
    "    final_schema = StructType(fields=data_schema)\n",
    "    for d in parse(path):\n",
    "        review = {}\n",
    "        review['asin'] = d['asin']\n",
    "        review['reviewerId'] = d['reviewerID']\n",
    "        review['rating'] = d['overall']\n",
    "        data.append(review)\n",
    "    return spark.createDataFrame(data, schema=final_schema)\n",
    "\n",
    "df_rating= getRatingData('Appliances.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1beb23ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 14:58:22 WARN TaskSetManager: Stage 4 contains a task of very large size (11992 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/15 14:58:29 WARN TaskSetManager: Stage 6 contains a task of very large size (11992 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert strings to vectors\n",
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in list(set(df_rating.columns)-set(['rating'])) ]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(df_rating).transform(df_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6b6ee4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 15:00:43 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:00:43 WARN TaskSetManager: Stage 8 contains a task of very large size (11992 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/15 15:00:45 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:00:45 WARN TaskSetManager: Stage 9 contains a task of very large size (11992 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/15 15:00:49 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:00:53 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:02 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:04 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:10 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/10/15 15:01:11 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "21/10/15 15:01:16 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:21 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:29 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:35 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:43 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:47 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:53 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:01:58 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:02:07 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:02:12 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:02:20 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#creating an item vector using ALS \n",
    "als=ALS(maxIter=5,regParam=0.09,rank=25,userCol=\"reviewerId_index\",itemCol=\"asin_index\",ratingCol=\"rating\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "model=als.fit(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e17fe4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/15 15:03:09 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:03:10 WARN TaskSetManager: Stage 55 contains a task of very large size (11992 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/15 15:03:13 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:03:15 WARN DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "21/10/15 15:03:22 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n",
      "21/10/15 15:05:58 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n",
      "21/10/15 15:07:46 WARN DAGScheduler: Broadcasting large task binary with size 20.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"rating\",predictionCol=\"prediction\")\n",
    "predictions=model.transform(transformed)\n",
    "rmse=evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019659e",
   "metadata": {},
   "source": [
    "### Get Similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "839e8849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_query(query_vec, category,vector_field, cosine=False):\n",
    "    \"\"\"\n",
    "    Construct an Elasticsearch script score query using `dense_vector` fields\n",
    "    \n",
    "    The script score query takes as parameters the query vector (as a Python list)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query_vec : list\n",
    "        The query vector\n",
    "    vector_field : str\n",
    "        The field name in the document against which to score `query_vec`\n",
    "    q : str, optional\n",
    "        Query string for the search query (default: '*' to search across all documents)\n",
    "    cosine : bool, optional\n",
    "        Whether to compute cosine similarity. If `False` then the dot product is computed (default: False)\n",
    "     \n",
    "    Note: Elasticsearch cannot rank negative scores. Therefore, in the case of the dot product, a sigmoid transform\n",
    "    is applied. In the case of cosine similarity, 1.0 is added to the score. In both cases, documents with no \n",
    "    factor vectors are ignored by applying a 0.0 score.\n",
    "    \n",
    "    The query vector passed in will be the user factor vector (if generating recommended items for a user)\n",
    "    or product factor vector (if generating similar items for a given item)\n",
    "    \"\"\"\n",
    "    \n",
    "    if cosine:\n",
    "        score_fn = \"doc['{v}'].size() == 0 ? 0 : cosineSimilarity(params.vector, '{v}') + 1.0\"\n",
    "    else:\n",
    "        score_fn = \"doc['{v}'].size() == 0 ? 0 : sigmoid(1, Math.E, -dotProduct(params.vector, '{v}'))\"\n",
    "       \n",
    "    score_fn = score_fn.format(v=vector_field, fn=score_fn)\n",
    "    \n",
    "    return {\n",
    "    \"query\": {\n",
    "        \"script_score\": {\n",
    "            \"query\" : { \n",
    "                \"bool\" : {\n",
    "                      \"filter\" : {\n",
    "                            \"term\" : {\n",
    "                              \"main_category\" : category\n",
    "                            }\n",
    "                        }\n",
    "                }\n",
    "            },\n",
    "            \"script\": {\n",
    "                \"source\": score_fn,\n",
    "                \"params\": {\n",
    "                    \"vector\": query_vec\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def get_similar(the_id, num=10, index=\"products\", vector_field='model_factor'):\n",
    "    \"\"\"\n",
    "    Given a item id, execute the recommendation script score query to find similar items,\n",
    "    ranked by cosine similarity. We return the `num` most similar, excluding the item itself.\n",
    "    \"\"\"\n",
    "    response = es.get(index=index, id=the_id)\n",
    "    src = response['_source']\n",
    "    if vector_field in src:\n",
    "        query_vec = src[vector_field]\n",
    "        category = src['main_category']\n",
    "        q = vector_query(query_vec, category,vector_field, cosine=True)\n",
    "#         print(q)\n",
    "        results = es.search(index=index, body=q)\n",
    "        hits = results['hits']['hits']\n",
    "        return src,hits[1:num+1]\n",
    "\n",
    "def display_similar(the_id, num=10, es_index=\"products\"):\n",
    "    \"\"\"\n",
    "    Display query product, together with similar product and similarity scores, in a table\n",
    "    \"\"\"\n",
    "    product, recs = get_similar(the_id, num, es_index)\n",
    "       \n",
    "    display(HTML(\"<h2>Get similar products for:</h2>\"))\n",
    "    display(HTML(\"<h4>%s (ASIN - %s)</h4>\" % (product['title'], product['asin'])))\n",
    "    display(HTML(\"<br>\"))\n",
    "    display(HTML(\"<h2>People who liked this product also liked these:</h2>\"))\n",
    "    sim_html = \"<table border=0>\"\n",
    "    i = 0\n",
    "    pd_data = []\n",
    "    for rec in recs:\n",
    "        r_score = rec['_score']\n",
    "        r_title = rec['_source']['title']\n",
    "        r = {}\n",
    "        r['asin'] = rec['_source']['asin']\n",
    "        r['title'] = r_title\n",
    "        r['score'] = r_score\n",
    "        pd_data.append(r)\n",
    "#         r_im_url = next(iter(rec['_source']['image']), '')\n",
    "#         sim_html += \"<tr><td><h5>%s</h5><img src=%s width=150></img></td><td><h5>%2.3f</h5></td></tr>\" % (r_title, r_im_url, r_score)\n",
    "        i += 1\n",
    "    sim_html += \"</table>\"\n",
    "    pd.set_option('display.max_colwidth', -1) \n",
    "    pd_df = pd.DataFrame (pd_data)\n",
    "    display(HTML(pd_df.to_html()))\n",
    "    display(HTML(sim_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e109a03e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1176/3751679919.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B000P9BY2E'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1176/2617184905.py\u001b[0m in \u001b[0;36mdisplay_similar\u001b[0;34m(the_id, num, es_index)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mDisplay\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtogether\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<h2>Get similar products for:</h2>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "data = display_similar('B000P9BY2E', num=5)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd74f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
